{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import bs4\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import signal\n",
    "\n",
    "from itertools import islice   \n",
    "\n",
    "\n",
    "import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBSFromURL(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers = CONFIG.HEADER)\n",
    "        return getBS(r.text)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Connection Error\")\n",
    "\n",
    "def getBS(html):\n",
    "    html_bs = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    return html_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple sleeping function to avoid getting banned\n",
    "def ImNotRobot(t = 5):\n",
    "    sleepTime = np.random.uniform(t,t+5)\n",
    "    print(f\"sleeping {sleepTime} seconds\")\n",
    "    time.sleep(sleepTime)\n",
    "    \n",
    "    \n",
    "def save():\n",
    "    all_issues_df.to_csv(all_issues_file_name, index = False)\n",
    "    \n",
    "def saveAndQuit(sig=None, frame=None):\n",
    "    save()\n",
    "    sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the scraping first\n",
    "\n",
    "To avoid error occuring during the parsing, I scrape all the html containing the article information first.\n",
    "Abstracts and author lists are packed in the webpage of an issue. Extraction of data will be handled afterweard once the raw html is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>scraped</th>\n",
       "      <th>bs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [URL, Volume, Number, Date, scraped, bs]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_articles_file_name = f\"{CONFIG.JOURNAL_NAME}.csv\"\n",
    "all_issues_file_name = f\"{CONFIG.JOURNAL_NAME}_all_issue.csv\"\n",
    "\n",
    "with open(all_issues_file_name) as f:\n",
    "    all_issues_df = pd.read_csv(f)\n",
    "needed_issues = all_issues_df[all_issues_df['scraped'] != 1]\n",
    "display(needed_issues)\n",
    "num = len(needed_issues)\n",
    "counter = 1\n",
    "\n",
    "for index, issue in needed_issues.iterrows():\n",
    "    url = CONFIG.DOMAIN_URL + issue['URL']\n",
    "    print(f\"Scraping {counter} / {num}\")\n",
    "    try:\n",
    "        issue_bs = getBSFromURL(url)\n",
    "        issue_main_bs = issue_bs.select('.table-of-content__section-body')[0]\n",
    "        all_issues_df.loc[index, 'bs'] = str(issue_main_bs)\n",
    "        all_issues_df.loc[index, 'scraped'] = 1\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"Terminated.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        all_issues_df.loc[index, 'scraped'] = -1\n",
    "        continue\n",
    "    finally:\n",
    "        all_issues_df.to_csv(all_issues_file_name, index = False)\n",
    "        counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Data\n",
    "\n",
    "Now with all the raw data scraped, we are ready to do the parsing. We iterate through the issues\n",
    "1. get the Volume , Number, Month, Year for the issue\n",
    "2. turn the bs string into soup\n",
    "3. select the ones with author\n",
    "4. get their parents\n",
    "5. get author elements\n",
    "6. try get abstract. Return empty is none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseVolume(v):\n",
    "    v = v.strip()\n",
    "    return v.split('Volume ')[1]\n",
    "\n",
    "def parseNumber(n):\n",
    "    n = n.strip()\n",
    "    return n.split('Number ')[1]\n",
    "\n",
    "def parseDate(d):\n",
    "    d = d.strip()\n",
    "    \n",
    "    ## split by comma first\n",
    "    split_comma = d.split(',')\n",
    "    \n",
    "    ## like Oct., 1996\n",
    "    if len(split_comma) == 2:\n",
    "        month = split_comma[0]\n",
    "        year = split_comma[1]\n",
    "        \n",
    "        return month.strip(), year.strip()\n",
    "    \n",
    "    ## else is like October 1998\n",
    "    split_space = d.split(' ')\n",
    "    if len(split_space) != 2:\n",
    "        return '', ''\n",
    "    \n",
    "    month = split_space[0]\n",
    "    year = split_space[1]\n",
    "    \n",
    "    return month, year\n",
    "\n",
    "def getArticlesWithAuthor(s):\n",
    "    c_articels = s.select('.issue-item__loa')\n",
    "    if not c_articels:\n",
    "        return None\n",
    "    return [a.parent for a in c_articels]\n",
    "\n",
    "def getArticleName(s):\n",
    "    c_issue_name = s.select('.issue-item__title a')\n",
    "    if not c_issue_name :\n",
    "        return ''\n",
    "    return c_issue_name[0].text\n",
    "\n",
    "def getArticleURL(s):\n",
    "    c_issue_url = s.select('.issue-item__title a')\n",
    "    if not c_issue_url :\n",
    "        return ''\n",
    "    return CONFIG.DOMAIN_URL + c_issue_url[0]['href']\n",
    "\n",
    "\n",
    "def getAbstract(s):\n",
    "    c_abstract = s.select('.accordion__content p')\n",
    "    if not c_abstract:\n",
    "        return ''\n",
    "    return c_abstract[0].text\n",
    "\n",
    "def getAuthors(s):\n",
    "    c_authors = s.select('.hlFld-ContribAuthor')\n",
    "    if not c_authors:\n",
    "        return []\n",
    "    return [\" \".join(a.contents[0].split()) for a in c_authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "with open(all_issues_file_name) as f:\n",
    "    all_issues_df = pd.read_csv(f)\n",
    "\n",
    "all_article = []\n",
    "for index, issue in all_issues_df.iterrows():\n",
    "    print(index)\n",
    "    volume = parseVolume(issue['Volume'])\n",
    "    number = parseNumber(issue['Number'])\n",
    "    month, year = parseDate(issue['Date'])\n",
    "    \n",
    "    issue_bs = getBS(issue['bs'])\n",
    "    articles = getArticlesWithAuthor(issue_bs)\n",
    "    for article in articles:\n",
    "        title = getArticleName(article)\n",
    "        url = getArticleURL(article)\n",
    "        abstract = getAbstract(article)\n",
    "        authors = getAuthors(article)\n",
    "        \n",
    "        to_append = [\n",
    "            url, title, volume, number, month, year, abstract\n",
    "        ] + authors\n",
    "        all_article.append(to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num = max([len(a) for a in all_article])\n",
    "\n",
    "colName = [\n",
    "    'Source URL',\n",
    "    'Title',\n",
    "    'Issue Vol.',\n",
    "    'Issue No.',\n",
    "    'Month',\n",
    "    'Year',\n",
    "    'Abstract'\n",
    "] + [f'Author {i+1}' for i in range(col_num - 7)]\n",
    "\n",
    "all_article_df = pd.DataFrame(all_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_article_df.to_csv(f'{CONFIG.JOURNAL_NAME}.csv', header=colName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = getBS(all_issues_df.loc[300,'bs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rosemary Avery', 'Donald Kenkel', 'Dean R. Lillard', 'Alan Mathios']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAuthors(getArticlesWithAuthor(bst)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
